\chapter{Learning}
\label{learning}
%generelt om learning
	For an agent to be able solve a problem without prior or very little knowledge, it needs to be able to learn how to handle things it has not seen before. To do this the agent needs experience to learn from, there are various ways of obtaining this experience and how to use it. In the two extreme ends of the spectrum are the two approaches "optimisic" and "pessimistic". which is best depends on the goal of learning and the environment.
% \cite{jacobsen2015a}
	
	\textbf{Optimistic} is build around everything being possible, unless proven otherwise. It focus on exploring the environment by trial and error. This approach works best if all actions in the environment is reversible or it can be reset. This is due to that the agent while exploring the domain inevitable will do something irreversible, (possible resulting in a non solvable level). It is also helpful if the environment can be modified to help exploring hypothesis about the environment.
		
	The approach can be compared to a "breath first search", and it will learn a wide array of possibilities in the domain, this wide knowledge helps finding solutions of a high quality. however as with BFS it is also slow, especially since time can be wasted on irrelevant knowledge but effective once learned.


	An advantage with this approach is that everything is assumed possible unless proven otherwise. This feature is especially advantageous on large domains where only a small subset of objects effects any given action. It also means that it will always find a plan but it can find faulty plans, which means it can consist of impossible actions or that the result of the plan would deviate from the predicted result. That results of a plan can be guaranteed means it will invertible end in undesired or unsolvable states. 
				
				%independent, dosnt need help from the outside....

		
		
	\textbf{Pessimistic} this approach is the polar opposite, where nothing is excepted to work unless proven otherwise. It is build around only doing actions that are guaranteed to work (already knows how to do), the agent will therefore never do something new. The only way of gaining new knowledge are from receiving and analysing traces, since the approach dosnt do something new, the traces it self can produce are not that helpful. the main way to obtaining these would be to ask for help from a "teacher" each time it is stuck as suggested in \cite{Action-Schemas}.
	The advantage of this approach is that everything the agent does is guaranteed to turn out as expected, and it will therefore never produce a faulty plan. It can however not guarantee to find a plan without the teacher. The approach can be compared to a "depth first search" due to it exploring in a single direction (know action), which means it will not necessarily find the best solution. 
	This approach learns quickly what it already have seen but only that, and the fact that everything always turns out as expected means it is useful if the environment is irreversible.
	
	

	
%	when looking a the astro kid domain its worth noting that its in fact using "both" when interacting 
%	bruge en kombination af pessimistisk og optimistisk....
%		pessimistisk ved tutorials
%		optimistisk p√• selve banen		
		
%	use the tutorial traces to speed up the learning
		
	
	\section{Learning in Astro Kid}
	It has been shown earlier that to solve problems in the Astro Kid domain using Fast Downward and PDDL, it is not feasible to model the complete world, and that it has to be relaxed. Therefore the relaxed version of the domain will be used for the purpose of learning. The PDDL states in them self can be considered as an interpretation of visual environment since the domain is fully observable. It is furthermore assumed that the possible actions names and parameters are given. This information are more or less equivalent to a human player knowing what the controls of the game are. 
	
	
	To simplify the learning problem, there will initially be started with learning a restricted version of the domain and extending from that. This initial restricted version will consist of the basic actions: left, right, up down and push (no continuous non player movement), since they in there simplest form can be described with solely conjunctions. This means that only parts of the domain are learned and the remaining is left as it is (destroy, gates etc.). The Action cost feature of PDDL is not considered and will be hard coded. However in general the action cost can be considered as an effect and learned in mostly the same way.

	

	When looking a the Astro Kid domain it is worth noting that it has a slow learning curve. Where most of the actions are introduced by a trace in a tutorial. Using these traces can speed up the learning, by quickly narrow down the search space, and give hints to what useful. The traces are therefore a good place to start for a pessimistic approach, they are however not complete enough to solely base a pessimistic approach on (e.g falling is not introduced).



	%are p(x,x) considered legal or only p(x,y)? 

\section{Basic learning}
	\label{basic}
	The learning algorithm used is based around observe hypothesise experiment. (algorithm \ref{mainAlgo}). It works by analysing a series of action in the domain, and from the knowledge obtained creating a hypothesise of how the world is believed to work, finally experimenting to see what happens when it is applied on the domain. This process is repeated until the enough is learnt to solve a given problem.

	
	\begin{algorithm}
		\caption{Learning algorithm}
		\label{mainAlgo}
		\begin{algorithmic}[1]
			\While{!goal}
			\State Analyse given trace
			\State make an hypothesis about how the domain works
			\State run planner with hypothesis
			\State get trace from simulation
			\EndWhile
			
		\end{algorithmic}
	\end{algorithm}	
	
%	the main learning is
%	
%	most basic conjunction.....
%	
%	the most essential parts of movement and push can be described solely using conjunction


	The main idea of how to analyse these traces and learn from them, is that if an action is applied on the domain and something happens, what happens must be the effect of that action, and that the action is applicable means that the preconditions of that actions is fulfilled. It can thereby be concluded that only the literal present before the actions is applied are part of the precondition. An action applied on the domain is considered a applicable as long as the simulations does not reject it. This means an action does not have to have a visible effect to be applicable. This is due to masked knowledge, this is the case where the effect of an action already being present in the domain, and therefore no changes occur when the same effect is applied again. This also means that it is not necessarily possible top find all effects of an action, due to the masked knowledge. In the same way as effects can be deduced so can what is not effects also be found.
	
	An action failing does not give the same amount of information, since it is not known which precondition is the cause of failure. However by combining knowledge from several attempts some information can be deduced. The idea is that at least one of the predicate not present is a precondition. The set of non present predicates is referred to as a candidate precondition. A candidate can be reduced by removing those predicates present at a success full actions. Only when reduced to a single predicate, can it be guaranteed that predicate is a precondition, that leaves the problem of if the action failed due to multiple precondition violations. If this is the case the best the a candidate preconditions can do is giving a hint to what the preconditions could be.
	
	
	When looking at the predicates present or not a special kind of predicate exists Equality, even though it differentiated from the classic predicates in how it is evaluated it can be deduced in the same way.
	 

	
	
	
%	the predicates can be grouped into three groups
%		proven
%		disproven
%		unknown
%		
%	action 	
%		succeeds
%			only tells what are not preconditions
%			
%		failure
%			tells what could be preconditions (missing candidates)
%				candidates
%			tells one situation that dindt work
%	
%

	
	
			
	When deducing the preconditions typing reduces the problem since it simplifies the grounding of the possible predicates (eg. in a domain  with 100 location, 1 stone, the predicate (at ?thing ?location) can be grounded 100 different ways, and without typing $101 ^ 2 = 10201$ different ways), however in worst case all objects would be of the same type, which would make the typing meaningless, and would therefore have no effect on the algorithm. In a properly typed domain the complexity would often be reduced considerably. However even with typing, grounding can be a problem since there is no restriction on the number of objects and how they can be relate to each other. It can clearly be seen with the positions and there relation to each other (a 10x10 map means 100 locations and 4 directions and their relative position defined by the predicate (relative-dir ?location ?loaction ?direction) can be grounded $(100 ^ 2 = 10000)*4=40000$ different ways).
	
	One way to further reduce the grounding problem is to assume that only predicates present (with that particular grounding) and the negated equivalent in the problem description are relevant, until proven possible by an effect. The idea is to avoid taking various impossible mutations of static objects into consideration. This is especially effective on problems with a underlying fixed structures, such as maps and levels. %The negated predicate is need to that .... walk left impossible, then right still possible
	
	This leaves how to handle "new" predicates not seen before created by effects. When a new predicate is discovered the negated version is add to all candidate preconditions. This however means that a candidate preconditions cant be proven unless all predicates is discovered. 


	Its worth remembering that even though a particular grounding influence if a given action is applicable at that moment, the grounding in it self and the type variables is not what is interesting, it is the relation between the predicates and therefore generalising of the parameters is need, to easier detect relations between predicates it is not enough just to know if a given predicate is present, but also what its relation to the other predicates is. A graph representation is therefore used, what is represented is how the objects is related to each other through known predicates.


\section{Generating the action schema}
One thing is to acquire knowledge from traces, the information also needs to be used, this is where the optimistic and pessimistic approach is required. They are used to generate the action schema. Due to the lack of a teacher an the optimistic approach is more advantageous, since it self-sufficient and it is possible to reset the environment. The preconditions and effects using the optimistic can be generated as follows. 


\textbf{Preconditions}
The preconditions is generated from the narrowed down candidate preconditions, even though that it is not known which part of the candidate preconditions that is the precondition, it is know that the particular combination fails. This can be used when generating the action schema, by adding all the candidate set as preconditions. when the sets are reduced enough the number of candidates can compensate for the lack of precision. This also ensures that no mistake is repeated


\textbf{Effects}
Everything happens unless proven otherwise,(every possible effect are applied unless the effect is disproven), this can be meaningless sometimes since the effects of not x and x would remove each other. To avoid this problem, each time both effects are applicable one is chosen by 50/50 chance.

\section{How does it work?}
\label{toFree}
Applying the learning approach using an optimistic works quiet badly due to how Fast Downward, handles the large number of unrestricted parameters, in the earlier stages of the learning. It is in particular the preprocessing which takes to long. This is not a problem solely for Fast downward, the YAHSP\footnote{\url{http://v.vidal.free.fr/onera/\#yahsp} 2. at IPC 2004} planner have the same problem except it dies on 5 parameters instead of 6 (results can be seen in  table \ref{un-table}).

\begin{table}
	\centering
	\caption{Unristrected search using level 1 and \ref{unrestricted}}
	\label{un-table}
	\begin{tabular}{llllll}
		Parameters & 2 & 3 & 4 & 5 & 6 \\
		YAHSP& 0.00 & 0.01  & 1.49 & x & x\\
		Fast Downward & 0.00 & 0.02 & 48 &  & x \\

	\end{tabular}
\end{table}
\begin{figure}
	\caption{Unrestricted action}
	\label{unrestricted}

\begin{lstlisting}
(:action Walk
	:parameters (
		?p0 - player
		?p1 - location
		?p2 - location
		?p3 - location
		?p4 - location
		?p5 - direction
	)
	:precondition 	(and
	)
	:effect 	(and
		(at ?p0 ?p2)
	)
)
\end{lstlisting}
\end{figure}


These results means that if learning is going to be applied using Fast Downward, it needs to be a more pessimistic approach, that restricts the parameters and thereby simplifies the preprocessing. Using a completely pessimistic approach leaves one problem, that there is in fact no teacher to ask for help. Some traces can be obtained from the tutorials, but they are not complete enough to solely rely on. If they are processed in order they occurred (falling is missing in the first tutorial).  

The pessimistic schema is is generated as follows.\\
\textbf{preconditions}
all possible preconditions, unless they are disproven.\\
\textbf{effects}
all proven effects.\\

To solve the problem the solution lies somewhere in between pessimistic and optimistic. The idea is therefore to relax the pessimistic approach little by little, when it cant find a solution, and thereby ensuring that some restrictions still exists. This is done by adding effects and removing preconditions. The effects add are selected from what effects could have been masked and each precondition is removed with a likely hood of 1/n (proven preconditions cant be removed). The main disadvantages of this approach is that some of the pessimistic approach still remains, which means that it is possible that it turns up no plan and as a result of that no new information can be learned from the experiment. To minimize the chance of this happening often the chance of changes increase with each no plan found. The difficult part here is in choosing the correct increase. Furthermore to avoid making the same the mistakes multiple times the preconditions from the optimistic approach is add. At least a single piece of new information is learned when a plan is found by relaxation no matter if the plan is valid or not. This is due to the worst case being the plan failing on the first action, however this still gives on failed action to deduce knowledge from. 

The approach for this limited version of the domain works well since given traces covers most of the domain, and what isn't covered can the relaxation compensate for. This can for example be seen on problem 1 (figure \ref{prob01}) and given trace walk left, only two predicate preconditions needs to be relaxed ((not (= ?p5 right)) and (= left ?p5) ) before the problem can be solved. If given the trace right instead it is solved without relaxation. The solution used concerning the grounding have a great impact on the relaxation since it removes a lot of non useful predicates eg. in case of the previous example it removes 69 predicates, such as (not (relativ-dir pos-01-01 pos-02-02 left))) (the action with and wiout the grounding filter can be seen in Appendix \ref{grounding}). This have effect that the relaxation effecting the correct predicate increases, and thereby speeds up the learning.


\section{Extending the domain}
	To further extend the support for the domain and allow a wider array of actions such as sliding. Conditionals and Disjunction needs to be add, however a working domain without these features can be created, but doing so would defeat the purpose of the learning by conforming the domain to the learning and not the other way around, by moving away from directly representing the controls available, used for controlling the avatar, and also quickly become unreadable.
		
\subsection{Conditionals}	
	When looking at conditionals they have the property that they can be compiled away by splitting the action into multiple actions, one for each conditional. This can done by moving the condition from effects to precondition and treating it as a non conditional action. at the same time adding conditions so only one of the new actions is applicable at any time. If a conditional have multiple effects they can also be considered separately.
	%example (better example)
	$p \land q \implies x \land z$
	is equivalent to 
	$(p \land q \implies x)
	\land
	(p \land q \implies z)$. 
	This means that each effect of an action can be considered as a separate sub-action with a single effect. So as soon as a given effect is present in a trace, it can be used for deducing the precondition for that effect, using the earlier described approach. 
	
	The method for deducing preconditions relies on that it is possible to connect a unique effect to its preconditions, therefore it needs the assumption that any given effect can at most appear once as an effect in each action.
	
	When the knowledge is deduced the trick is then to merge all the separate sub-actions when generating the action schema, for the pessimistic approach this is done by using the intersection of all candidate preconditions as preconditions and the difference as conditionals.
	
	
\subsubsection{Disjunction}
	The base (non disjunction part) of an action can be deduced as earlier described (section \ref{basic}), the disjunction part cant be found this way, since the different parts of the disjunction would eliminate each other. Using this approach, it is however possible to detect if a disjunctions appears in the action, this can be seen when contradictions happens (an action failing when fulfilling all the predicted precondition). When such a contradiction to the base happens it is guaranteed that at least one predicate from the disjunction precondition is not presents. When only looking at the simplest version of disjunctions of the form $ x \lor y \lor z...$ with a maximum of a single disjunction per. action. This simple disjunction can be deduced by using that all present predicates from a contradiction traces cant be part of the disjunction, since it would otherwise have succeed. Furthermore since it possible to treat each effect as an separate sub-action it is also possible using the same approach to deduce a single disjunction for each sub-action, which means a maximum of one disjunction precondition and one disjunction for each conditional. 
	
	This approach however falls apart as soon as more or more complex (eg nested conjunction) disjunction are allowed. The main problem here is that the used approach relies on that no predicates from the disjunction being present on failure where the base is fulfilled, and this can no longer be guaranteed eg. in $w \lor x \lor (y \land z )$ a failure with y present would remove y. To avoid this problem it is necessary to identify which part of the disjunctions are related to the failure or success. This is however difficult to determine due to uncertainty add by the disjunctions. The uncertainty is rooted in that multiple disjunctions can hide each others "effects". When looking at a failure it is therefore not possible to eliminate any predicates, since potentially any combination of the non present predicates could be part of a disjunction and therefore the cause of the failure. The only thing that can be conclude is that particular complete set of preconditions isn't allowed. On a success the same problem occurs. So without a way to connect course and effect it is basically down to something is missing from some disjunction. In the same note adding more complexed disjunctions to the domain also makes it nearly impossible to determine when an action is completely learned. This is due to that every combination of preconditions needs to be tried, unlike earlier where preconditions could be removed if not present a single time. This means that a different approach is need for action schemas using more complex disjunctions, however the simple approach should be sufficient to interact with the Astro Kid domain.
	 
	%more advanced disjunction leads to a violation of the assumption...
	

%	
%	avoid storing every single episode since complete knowledge would be need
	


	Deducing disjunction introduces a new aspect, that effects the learning process.  Which is that it is no longer possible to discard already learnt episodes,  without the risk of loosing knowledge. The problem lies in that knowledge are obtained from contradictions to the base, and some of these cant be discovered until more knowledge is obtained. Furthermore the speed of learning is also effected since the disjunction cant be started to be learn before contradiction can be detected. 
	
%	this also means that by learning more until a disjunction is detected  
	
	%this knowledge is discard since.... optimistic approach.... negligible knowledge loss.
	
	
\section{Generating the action schema}	
	
	The basic structure for generating the actions remains the same, the differences lies in that multiple sub-actions needs to be merged into a single one. This can be done by taking the intersection of the sub-actions preconditions as the preconditions, and having the difference as the conditionals(figure \ref{merge}). Whit the additions of disjunctions the approach moves slightly more away from the completely pessimistic approach. This is due to the disjunction part being deduced using an optimistic approach, which meant the generated disjunction being a disjunction of all possible disjunction predicates.
\begin{figure}
	\caption{Example of merging two subactions}
	\label{merge}
		\begin{lstlisting}
subaction 1
pre: (and (at ?p1 ?p2) (relativ_dir ?p2 ?p3 right) 
	(relativ_dir ?p3 ?p4 down))
effect: (and (at ?p1 ?p3))
		
subaction 2
pre: (and (at ?p1 ?p2) (relativ_dir ?p2 ?p3 right) 
	(relativ_dir ?p3 ?p4 down) (clear ?p4))
effect: (and (falling ?p1))
		
merged action
pre: (and (at ?p1 ?p2) (relativ_dir ?p2 ?p3 right) 
	(relativ_dir ?p3 ?p4 down))
effect: (and 
	(at ?p1 ?p3)
	(when (clear ?p4) 
		(falling ?p1)
	)
)
\end{lstlisting}
\end{figure}	

	
	
% quantifiers....
%	as with the more complex disjunctions the main problem is to get a handle on it.... a place to start...	
	
%	exstential quantifiers...

%the simple use of quantifiers in the domain can easily be deuced.... if an single predicate exists....

%	universal quantifiers...
	The presence of universal quantifiers can be detect if effects not associated with parameters are present. That the effect predicate isn't associated with parameters also gives a handle on where to start when deducing the preconditions (a single unique point). The handle means that the quantifier can be deduced nearly as any other effect. The two main differences is that the predicates considered is no longer only the subset containing the parameters but all present predicates. the second  difference is that the graph representations is essential. 
	
	
	universal quantifiers used as solely preconditions using imply can be transformed into existential quantifiers by inverting the set of conditions
%\section{space complexity}

	
\section{How does it work?}	
	The consequence of extending the domain is that learning an single action takes longer (number of episodes need)\textbf{table ....}. This is expected since that there are more possible outcomes of a given action and therefore also more to learn. The specific thing that slows down the learning is that single episode/trace cant no longer cover as large a percentage of an actions conditions and effect as earlier due to the complexity of the actions. The only place where learning conditionals have a slight effect on the speed is when an effect predicate consist of an literal which is both an parameter and a constant. An example of this can be seen when learning the action Walk (figure\ref{double}) its split into two subaction with the same effect in reality. This means the same thing is deduced twice, but the performance loss is negligible since the main bottleneck is finding traces to deduce from.  
	\begin{figure}
		\caption{Double work done when deducing actions}
		\label{double}
		\begin{lstlisting}
(:action walk
	:parameters  	(?p - player ?from ?to ?underFrom 
						?underTo - location ?dir - direction)
	...
	...
	...
	:effect 	(and
		(when (and
				(not (ground blue ?p4))
				...
				...
				(relativ-dir ?p1 ?p2 right)
			)
			(moving ?p0 ?p5)
		)
		(when (and
				(not (ground blue ?p4))
				...
				...
				(relativ-dir ?p1 ?p2 right)
			)
			(moving ?p0 right)
		)	
		...
		...
	)
)
		\end{lstlisting}
	\end{figure}
	
	Learning conditionals in it self have no effect on how many steps it takes to learn a action, when taking into consideration that, if the conditionals had been compiled away (into multiple action) it would be take longer since what is in common could no longer be learned simultaneously. The disjunction part however have an impact since the base have to be learned before the disjunction can be learned, the consequences of this mostly mitigated by the optimistic approach used for this part.
	
	
	
%	a little extra time is used for deducing since it is dependent on the number of sub actions... not important since minor and number of plans....
	When running the learning system three main situations appears, when not considering the unsolvable levels for the classic approach. The first is that enough knowledge is already know, and the level can be solved using the planner just as in classic approach. This is for example the case when reaching level 2 and already having solved level 1 and seen the corresponding tutorials. The second is where not enough knowledge is know and relaxation therefore is need to find new plans.
	
	
	As can be seen in \textbf{table ....} if to many new situations are introduced at the same time, it can be difficult to create traces even with relaxation. The problem lies in that multiple specific preconditions needs to be relaxed simultaneously and if one is missed no trace can be produced. Even more relaxation dosnt necessary solve the problem, since it then reintroduces the problem from section \ref{toFree}.
	
	The reason to why to many new situations are introduced at once, even despite slowly growing learning curve, is that for a human, many of the new situation dosnt require an explanation eg. when introduced to walking on ground, it obvious that the avatar can walk off an edge. Basically the learning approach is missing some background information to make assumptions about the world on. However even with this background knowledge it isn't even possible to use it for the learning since concepts as up, down and gravity dosnt translate to PDDL. This means that even though the tutorials for Astro Kid is complete enough for a human to understand the domain, this is not the case for learning using PDDL and some essential concepts is not explained. To compensate for this either more traces are need or a planner that allow a more optimistic approach.
	
	
%	the choice of filtering out not seen predicates reduces, have greatest effect on large number of parameters.... also helps with the relaxation, since it reduces the number of unrelated predicates and thereby increases the chance of relaxing the right...  

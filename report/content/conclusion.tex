\section{Conclusion}

%what has been achieved
the Astro Kid enviroment has been described in PDDL and integrated with Fast Downward in such a way that it is able to solve most of the levels in the domain with some limitations concerning concurrency. this has been combined with a learning process which enables it to to learn from given traces and to generates its own new traces when the given isnt sufficient. this is done using a mostly pessimistic approach to learning.


Working with Fast Downward have shown that general purpose planners still have some way to go before truly becoming general purpose planners. The problems with Fast Downward illustrates some of the weakness for general purpose planners and strengths. The main weakness is that the concept of a general purpose planner needs to be able to span a wide array of problems and sometimes problems not considered or prioritised by the creators of the planner. which it fails at....

it neither helps that the PDDL and the planners are driven which is driven by IPC competition. 
 since it limits the diversity..... 
 
 on the other hand when conforming to the type of problems the planner is expecting everything works well.
 
 one would expect that if straying a little away from IPC problems it would be need to optimise the problem description ..... knowledge of the particular planner is need
 
 
 
 
 something something learning........
 the learning system created works well.... but if it needs to be extend further a different approach is need
 
 
\subsection{Future Work}
%hvad skal der til for at pddl kan understøtte domainet optimalt....

	it would be interesting to see how the complete Astro Kid domain could be supported by PDDL. The main problem to achieve this is the concurrency created by the continuous actions. This could be solved by either extending the language support for temporal planning or switching to a multi agent version of PDDL. The idea of switching to a multi agent version fx. MA-PDDL is to threat all objects that can be involved in continuous actions and treating them as independent agents. This could help handling the concurrency issues and thereby allowing the planner to solve levels where this is an issue such as level 25. Switching to a multi agent system would however introduce noise (which agent caused what) and thereby more uncertainty to the system when considering learning. Since the chosen approach already is limited by uncertainty.
	
	a different approach in learning to dealing with uncertainty and to further extending the supported parts of PDDL is need. learning based on Markov logical network used in \cite{zhuo2010a} 
	 evolutionary algorithms... mmas? using basic approach to manipulate values
	
	
%hvad skal der til for  lærer  det der mangler qunantifiers, param
 to truly being able to just drop an agent in a new universe make it behave in it. the agent needs to be able to handle not knowing which parameters are useful.. so future work would be concentrated around learning from solely a snap shot of before and after an actions is applied. 

	
	
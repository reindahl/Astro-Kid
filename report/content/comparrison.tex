\section{Tests}

%quality
%
%domain
%level04.pddl
%translate+preprocess
%187.649
%0.984
%plan lenght: 35
%
%level25small.pddl
%translate+preprocess
%299.333
%5.4
%plan lenght: 32
%
%
%level07.pddl
%translate+preprocess
%514.073
%116.042
%plan lenght: 45
%
%no update
%
%level04.pddl
%translate+preprocess
%7.498
%0.248
%plan lenght: 35
%
%level25small.pddl
%translate+preprocess
%4.637
%no plan
%speed



	
	%the PDDL Domain description 
	%how does it handle when the possible action increase
	%how does it handle when the problem size increase


	There are various ways of tweaking the performance of the planner. The two most obvious ways of doing this, is changing the parameters of the planner and changing the PDDL domain to fit the planners strengths better.
	% comparrison

		\subsection{Plan Quality}
		% quality of the soloution
		
			%table of results.....
			%wich heuristic is used FF
			
						\begin{table}[h]
							\centering
							\caption{Plan Quality}
							\label{quality}
							\begin{tabular}{llllll}
								& level 4 & level 7  & level 25small & problem 24 & problem 24v2\\
								classic & 35 & 45 & x&11&11\\
								update& 35 &45& 32&7&11\\
							\end{tabular}
						\end{table}
			
			%run on toy problem that shows post processing
			Looking at results in table \ref{quality} it can be seen that on problems where the assumption holds the found solutions are equal. Some difference can be seen where the post processing is necessary (problem 24 and 24v2 where the robot is activated to late). The post processing can in most cases be optimised to perform close or equal to the update approach. However to achieve the best possible result from the post processing domain specific knowledge would be need.	A greater difference is seen on levels such as level 25, where the assumptions used in the classic approach fails completely (moving objects needs to hit each other). The number of levels where this is the case is fairly limited. As one would expect it can be seen that domain only effect the quality of the solution where the assumptions made dosnt hold.
			
			
		\subsection{Speed}
		% speed
			When looking at the data its worth taking note of that most of the time is not always spend in search, as one would expect for a planner, but instead in preprocessing/translating. This is interesting due to that most other planners, works more directly with the PDDL description, and therefore dosnt have to use the same amount of time on this step.
		
			A significant speed advantage to the classic approach can be seen on all problems it can solve. The difference in speed was expected, due to the domain type being relaxed and more similar to the ones used in the IPC. One would therefore expect the planner to be more optimized for this kind of problem.
			

			%why the large difference.....
			What wasn't expected was how large the difference. On a problem such as level 4 is the difference roughly a factor of 20, it is however worth taking note of that the speed difference isnt equal for all the sub parts of the search. It is in particular the translator which is the bottle neck. 
			
			%what can be done about it
			The great speed difference makes the update approach less useful. For the it to be able to solve more levels this speed difference needs to be reduced. Test have shown that use of universal quantifiers have a large impact on the translating/instantiation time. Therefore there is created a variation of the domain where a single universal quantifier is replaced by an existential quantifier and an extra action(s).(code can be found at Appendix \ref{Domain_Variation})
			%comparing existential and universal


			\begin{table}[h]
				\centering
				\caption{problem 4}
				\label{prob4}
				\begin{tabular}{lllll}
					& forall & simple  \\
					total Time & 26.2 & 136.2 \\
					translator Time& 21.9 & 6.6 \\
					
					
					relevant atoms & 8872 & 12968\\
					auxiliary atoms & 16959& 18807\\
					final queue length &25831 & 31775\\
					total queue pushes &58904 & 68537\\
					axioms & 1 & 132652 \\ 
					peak memory & 100568 KB & 208356 KB\\ 
					task size & 42751 & 573269\\
					
					
					preprocessing Time& 4.2 & 127.4 \\
					necessary operators & 7292 & 7337\\
					
					
					search Time & 0.1 & 2.2 \\
				\end{tabular}
			\end{table}
			%insert table of running times
			When running the different versions of the domain on various problems, one thing becomes clear, the total times varies greatly depending on the domain and level combination (table \ref{times}). However when looking at the results (table \ref{prob4}), what shows is that the use of universal quantifiers greatly increases the instantiation/translation time. 
			
			Replacing the universal quantifiers ensures a quicker instantiation/translation, but it has the cost of the number of axioms exploding, and in general the number of atoms growing by 5-20\%, which in the end can greatly hampers the preprocessing that generates the Causal graph.
			
			The tendency seams to be that the more movable objects and complex the problem the better the version with universal quantifiers does, this is especially clear when moving from toy problem to actual levels (tabel \ref{lvl4} and \ref{lvl9}). Generally this existential variation of the domain only really shines when the problem is "small" and isnt therefore useful on the different levels in Astro Kid.
			
			\begin{table}[h]
				\centering
				\caption{problem 4v2}
				\label{prob4v2}
				\begin{tabular}{lllll}
					& Universal & Existential  \\
					total Time & 9.8 & 5.2 \\
					translator Time& 7.1  & 2.2 \\
					
					
					relevant atoms & 6742 & 8199\\
					auxiliary atoms & 13900 & 14710\\
					final queue length & 20642 & 22909\\
					total queue pushes & 40653 & 44321\\
					axioms & 1 & 534 \\ 
					peak memory & 71356 KB & 66040 KB\\ 
					task size &29496 & 31067 \\
					
					
					preprocessing Time & 2.6 & 2.8 \\
					necessary operators & 5469 & 5485 \\
					
					
					search Time & 0.1 & 0.2 \\
				\end{tabular}
			\end{table}
			%wich domain
			An anomaly is shown in prob4v2 table \ref{prob4v2} (where a single stone is add to problem04) and not much difference would be expected, but here the planner figures out that the objects is in fact not movable, and the explosion of axioms dosnt happen. 
			
%			\subsection{Problemdefinition}
			Another way of tweaking the code is to optimize the Problem Definition, this can be done by removing unreachable states/objects, more precise remove the representation of position that isn't useful. The effect of this can clearly be seen when adding unused location to prob02 and the results are shown in table \ref{whitespace}. The results shows for each variation the time need roughly doubles. Interestingly enough the version of the domain with existential quantifiers isnt effected nearly as much by it (table \ref{times}). 
			one version scales with the size of the problem (grounding), the other with the number of objects (axiom explosion). This shows that long running times does not necessary correlate to a more complex problem. It also especially for universal quantifier version shows the weakness of a general planner, which is that it cant use specific knowledge of the domain, and therefore cant easily discard non relevant areas of the problem.
			
			
			\begin{table}[h]
				\centering
				\caption{level 4}
				\label{lvl4}
				\begin{tabular}{lllll}
					& Universal & existential  \\
					total Time& 175.4 & x \\
					translator Time& 164.4 &x  \\
					
					
					relevant atoms & 27782 & x\\
					auxiliary atoms & 40327 &x \\
					final queue length & 68109 &x \\
					total queue pushes & 201441 &x \\
					axioms & 13 & x \\ 
					peak memory & 231780 KB &x  KB\\ 
					task size & 138081 &x \\
					
					preprocessing Time& 10.0 &  x\\
					necessary operators & 24525 & x\\
					
					search Time & 1.0 &x  \\
				\end{tabular}
			\end{table}
			

						\begin{table}[h]
							\centering
							\caption{level 9}
							\label{lvl9}
							\begin{tabular}{lllllll}
								& classic  & Universal & existential\\
								total Time& 11.5 & 301.7 &x \\
								translator Time& 5.6& 280.3 & x\\
								
								
								relevant atoms & 8797& 61431 &x \\
								auxiliary atoms & 169254 & 64980 &x \\
								final queue length & 178051& 126411 &x \\
								total queue pushes & 746595& 359476 &x \\
								axioms &  265& 23 &x \\ 
								peak memory & 195568 KB& 386872 KB &x \\ 
								task size & 43197& 265076 &x \\
								
								
								preprocessing Time& 5.4 & 17.2 &x \\
								necessary operators & 4283 & 49496 &x \\
								
								
								search Time & 0.5 & 4.2 &x \\
							\end{tabular}
						\end{table}
			
			
			
			
			\begin{table}[h]
				\centering
				\caption{white space. for each version the width of the problem have been increased by 5}
				\label{whitespace}
				\begin{tabular}{lllll}
					& prob02 & prob02v2 & prob02v3 & prob02v4\\
					total Time & 11.2 & 24.0 & 49.3 & 91.0\\
					translator Time& 7.6 & 17.8 & 38.8  & 75.5\\
					
					
					relevant atoms & 12394 & 22257 & 34922  & 50387\\
					auxiliary atoms &14389 & 20021 & 25656  & 31291\\
					final queue length &26783 & 42278 &60578  & 81678\\
					total queue pushes & 58980 & 101633 & 155153 & 219523\\
					axioms & 1 & 1  & 1 & 1 \\ 
					peak memory & 100800 KB & 157492 KB & 229452 & 317760\\ 
					task size & 56625 &  104505 & 166385 & 242265\\
					
					
					preprocessing Time& 2.4 & 5.8 & 9.9  & 24.5 \\
					necessary operators & 10774 & 20079  & 32184 & 47089\\
					
					
					search Time & 0.2 & 0.4 & 0.6  & 1.0\\
				\end{tabular}
			\end{table}
			%	\begin{table}[h]
			%		\centering
			%		\caption{level 4}
			%		\label{lvl4}
			%		\begin{tabular}{lllll}
			%			 		   & forall & simple  \\
			%			total Time &  &  \\
			%			translator Time&  &  \\
			%			
			%			
			%			relevant atoms & & \\
			%			auxiliary atoms & & \\
			%			final queue length & & \\
			%			total queue pushes & & \\
			%			axioms &  &  \\ 
			%			peak memory & KB &  KB\\ 
			%			task size &  & \\
			%
			%
			%			preprocessing Time& &  \\
			%			necessary operators & & \\
			%
			%
			%			search Time &  &  \\
			%		\end{tabular}
			%	\end{table}
			
			\begin{table}[h]
				\centering
				\caption{Running times}
				\label{times}
				\begin{tabular}{llllllllllllllllllll}
					& prob00 & prob01& prob02& prob02v2& prob02v3& prob02v4& prob02v5& prob03\\
					universal 	& 0.892  &0.887  &11.326 &24.798   &52.617   &95.08    & x       &0.794    \\
					existential &0.646   &0.788  &6.777  &12.148   &18.863   &27.405   &36.86    &0.704  \\
				\end{tabular}
				\begin{tabular}{llllllllllllllllllll}
					&  prob04& prob04v2& prob07&  prob09& prob10& prob11& prob12 & level 4\\
					universal    &23.905  &9.197      &36.326   &5.842  &13.45  &2.4 2           &21.484 &429.9\\
					existential      &124.794 &4.434   &14.228 &2.444  &5.789  &1.713 s        &7.346 s & x\\
				\end{tabular}
			\end{table}


\subsection{Heuritics}
%todo test data of different heuristics
The main parameter in Fast Downward that can be tweaked is the heuristic. This choice of heuristic is fairly limited due most of the them supported by fast downward dosnt support the use of axioms or conditional effects, which are used in the domain description, and those heuristics that does, do so barely\footnote{(in the sense that the planner won't complain -- handling of axioms might be very stupid and even render the heuristic unsafe) \url{http://www.fast-downward.org/Doc/Heuristic}}. The only supported heuristic that is admissible is blind, and as the name suggest isnt the most advanced of the heuristics.


The importance of the choice heuristic varies greatly depending on the level. and if speed, quality or a combination is wished for. For the update approach choice of heuristic is fairly unimportant since the bottleneck is at the translator/preprocessing. When looking at the classic approach this changes. The effect of the heuristic on the different levels varies greatly, fx. on level 4 the time consumed by the search is minor.

The heuristics blind and context enhanced additive both does well on all the level except a single one each. 

The additive heuristics is consistently fast but gives a solution of poor quality.

The blind heuristic gives a high quality due to it being admissible and it is fast except on a single level where it is 60 time slower than the second to last.

The simplest possible explanation for why the some heuristics does terrible on some levels, is that they cant handle axioms well and will handle certain situations "stupid". This cant explain why Blind fail, because it handles this type well due to its simplicity. The reason would rather be that it is a blind search, and is therefore prone to explore the state space less efficient direction some time. Therefore it is not really unexpected that blind fails miserable on a level, but more that it dosnt fails on more. That this does not happen more often, must be credited to the preprocessing of the problem.






%heuristics
%	admissible
%		blind
%
%
%	additive
%	Context-enhanced additive
%	causal graph
%	FF
%	max

	\begin{table}[h]
		\centering
		\caption{level 4}
		\label{tablvl4}
		translate+preprocess
		7.425 s\\
		\begin{tabular}{lll}
			name & time (s)& lenght\\
			Additive & 0.329 & 35\\
			blind & 0.196 & 35\\
			ContextEnhancedAdditive & 0.193 & 35\\
			causalGraph & 0.182 & 35\\
			ff & 0.19 & 35\\
			max & 0.188 & 35\\
		\end{tabular}
	\end{table}
	
	\begin{table}[h]
		\centering
		\caption{level 7}
		\label{tablvl7}
		translate+preprocess
		8.086 s\\
		\begin{tabular}{lll}
			
			name & time (s)& lenght\\
			add & 1.5 & 77\\
			blind & 14.8 & 76\\
			Context-enhanced additive & 13.1 & 76\\
			causal graph & 17.47 & 76\\
			ff & 22.0 & 76\\
			max & 26.9 & 76\\
		\end{tabular}
	\end{table}
	
	\begin{table}[h]
		\centering
		\caption{level 10}
		\label{tablvl10}
		translate+preprocess
		21.248 s\\
		\begin{tabular}{lll}
			
			
			name & time (s)& lenght\\
			Additive & 0.878 & 34\\
			blind & 2.174 & 34\\
			ContextEnhancedAdditive & 0.717 & 34\\
			causalGraph & 1.024 & 34\\
			ff & 0.755 & 34\\
			max & 1.326 & 34\\
		\end{tabular}
	\end{table}
	\begin{table}[h]
		\centering
		\caption{level 14}
		\label{prob4}
		translate+preprocess
		12.389 s\\
		\begin{tabular}{lll}
			
			
			name & time (s)& lenght\\
			Additive & 12.719 & 273\\
			blind & 11.367 & 158\\
			ContextEnhancedAdditive & 104.799 & 181\\
			causalGraph & 20.722 & 158\\
			ff & 33.153 & 158\\
			max & 28.949 & 158\\
		\end{tabular}
	\end{table}
	
	\begin{table}[h]
		\centering
		\caption{level 32}
		\label{tablvl32}
		translate+preprocess
		24.389 s\\
		\begin{tabular}{lll}
			
			name & time (s)& lenght\\
			Additive & 0.934 & 20\\
			blind & 131.198 & 17\\
			ContextEnhancedAdditive & 0.712 & 17\\
			causalGraph & 2.213 & 17\\
			ff & 0.748 & 17\\
			max & 0.961 & 17\\
		\end{tabular}
	\end{table}
\subsection{Remarks on general planning and Astro Kid}
	The Astro kid world can be represented in PDDL, but the domains use of continuous actions, forces the representation to either be relaxed, or made in an awkward way, where the planner have trouble handling it. For PDDL to handle the Astro Kid domain properly, there needs to be introduced a proper way of handling concurrency.

	Relaxing the domain ensures that a solution is found quickly, but this solutions is not always valid. This can in some cases be solved by adding noOps at appropriate places. Adding noOps to the the plan when necessary takes a little away form the idea of using the general planner, by requiring some specialised knowledge about the domain, that cant easily be reused on other domains.
	
	The Update approach guaranties to give a valid optimal solution, due to simulating the complete domain. This approach however have the cost of time and memory used. This is due to the domain not fitting well into the classical domain. The variations of this approach shows this clearly by also hitting into problems. In general the update approach is only useful on domain where the assumption for the classic approach, due to large speed difference an nearly equal quality of the found plans. Even then it only works on simple domains, since it often fails to find a solution due to finite memory and time constraints.
	
	The heuristics causal graph, ff and max have shown to be the most reliable of the available heuristics with no significant difference between them.
	

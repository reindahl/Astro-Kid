\section{learning}
%generelt om learning
	for an agent to be able solve a problem without prior or very little knowledge, it needs to be able to learn how to handle things it has not seen before. To do this the agent needs experience to learn from, there are various ways of obtaining this experience and how to use it. In the two extreme ends of the spectrum are the two approaches "optimisic" and "pessimistic". which is best depends on the goal of learning and the environment.
% \cite{jacobsen2015a}
	
	\textbf{Optimistic} is build around everything being possible, unless proven otherwise. It focus on exploring the environment by trial and error. This approach works best if all actions in the environment is reversible or it can be reset. This is due to that the agent while exploring the domain inevitable will do something irreversible, (possible resulting in a non solvable level). It is also helpful if the environment can be modified to help exploring hypothesis about the environment.
		
	The approach can be compared to a "breath first search", and it will learn a wide array of possibilities in the domain, this wide knowledge helps finding solutions of a high quality. however as with BFS it is also slow, especially since time can be wasted on irrelevant knowledge but effective once learned.


	An advantage with this approach is that everything is assumed possible unless proven otherwise. This feature is especially advantageous on large domains where only a small subset of objects effects any given action. It also means that it will always find a plan but it can find faulty plans, which means it can consist of impossible actions or that the result of the plan would deviate from the predicted result. That results of a plan can be guaranteed means it will invertible end in undesired or unsolvable states. 
				
				%independent, dosnt need help from the outside....

		
		
	\textbf{Pessimistic} this approach is the polar opposite, where nothing is excepted to work unless proven otherwise. It is build around only doing actions that are guaranteed to work (already knows how to do), the agent will therefore never do something new. The only way of gaining new knowledge are from receiving and analysing traces, since the approach dosnt do something new, the traces it self can produce are not that helpful. the main way to obain the would there for to ask for help from a "teacher" \cite{Action-Schemas}.
	The advantage of this approach is that everything the agent does is guaranteed to turn out as expected, and it will therefore never produce a faulty plan. It can however not guarantee to find a plan without the teacher. The approach can be compared to a "depth first search" due to it exploring in a single direction (know action), which means it will not necessarily find the best solution. 
	This approach learns quickly what it already have seen but only that, and the fact that everything always turns out as expected means it is useful if the environment is irreversible.
	
	

	
%	when looking a the astro kid domain its worth noting that its in fact using "both" when interacting 
%	bruge en kombination af pessimistisk og optimistisk....
%		pessimistisk ved tutorials
%		optimistisk p√• selve banen		
		
%	use the tutorial traces to speed up the learning
		
	
	\subsection{Learning in Astro Kid}
	It has been shown earlier that to solve problems in the Astro Kid domain using Fast Downward and PDDL, it is not feasible to model the complete world, and that it has to be relaxed. Therefore the relaxed version of the domain will be used for the purpose of learning. The PDDL states in them self can be considered as an interpretation of visual environment since the domain is fully observable. its furthermore assumed that the possible actions names and parameters are given. This information are more or less equivalent to a human player knowing what the controls of the gamee are. 
	
	
	To simplify the learning problem, there will initially started with learning a restricted version of the domain and extending from that. This initial restricted version will consist of the basic actions: left, right, up down and push (no continuous non player movement). This means only parts of the domain are learned and the remaining is left as it is (destroy, gates etc.). The Action cost feature of PDDL is not considered and will be hard coded. However in general the action cost can be considered as an effect and learned in mostly the same way.

	
	
	
	When looking a the Astro Kid domain its worth noting that it has a slow learning curve. with tutorials where most of the actions are introduced by a trace. Using these traces can speed up the learning, by quickly narrow down the search space. These traces are also in them self a good place to start for a pessimistic approach, they are however not complete enough to solely base a pessimistic approach on (e.g falling isnt introduced).



	%are p(x,x) considered legal or only p(x,y)? 

\subsection{Basic learning}
	\label{basic}
	The learning algorithm used is based around observe hypothesise experiment. (algorithm \ref{mainAlgo}) It works by analysing a series of action in the domain, and from the knowledge obtained creating a hypothesise of how the world is believed to work, finally trying/experimenting to see what happens when its applied on the domain. this process is repeated until the enough is learnt to solve a given problem.
	
	
	
	
	\begin{algorithm}
		\caption{Learning algorithm}
		\label{mainAlgo}
		\begin{algorithmic}[1]
			\While{!goal}
			\State Analyse given trace
			\State make an hypothesis about how the domain works
			\State run planner with hypothesis
			\State get trace from simulation
			\EndWhile
			
		\end{algorithmic}
	\end{algorithm}	
	
	the main learning is
	
	most basic conjunction.....
	
	the most essential parts of movement and push can be described solely using conjunction


	The main idea of how to analyse these traces and learn from them, is that if an action is applied on the domain and something happens, what happens must be the effect of that action, and that the action is applicable means that the preconditions of that actions is fulfilled. It can thereby be concluded that only the literal present before the actions is applied are part of the precondition. An action applied on the domain is considered a applicable as long as the simulations dosnt reject it. this means an action dosnt have to have a visible effect to be applicable. this is due to masked knowledge, this is the case of the effect of an action already being present in the domain, and therefore no changes occur when the same effect is applied again. this also means that it is not necessarily possible top find all effects of an action, due to the masked knowledge.
	
	An action failing does not give the same amount of information, since it is not known which precondition is the cause of failure. However by combining knowledge from several attempts some information can be deduced. The idea is that at least one of the predicate not present is a precondition, the list of possible candidates can be narrowed down by comparing with those present at a success full action. only when narrowed down to a single candidate, can it be guaranteed that candidate is a precondition, that leaves the problem of if the action failed due to multiple precondition violations. if this is the case the best the set of candidate preconditions can do is giving a hint to what the preconditions could be.
	
	
	predicates inst the only thing that can be a precondition....
	The equality precondition can be treated as any other predicate
	
	in the same way as effects can be deduced so can what is not effects also be found. 

	
	
	
%	the predicates can be grouped into three groups
%		proven
%		disproven
%		unknown
%		
%	action 	
%		succeeds
%			only tells what are not preconditions
%			
%		failure
%			tells what could be preconditions (missing candidates)
%				candidates
%			tells one situation that dindt work
%	
%

	
	
			
	When deducing the preconditions typing reduces the problem since it simplifies the grounding of the possible predicates	(eg. in a domain  with 100 location, 1 stone, the predicate (at ?thing ?location) can be grounded 100 different ways, and without typing $101 ^ 2 = 10201$ different ways), however in worst case all objects would be of the same type, which would make the typing meaningless, and would therefore have no effect on the algorithm. In a properly typed domain the complexity would often be reduced considerably. However even with typing, grounding can be a problem since there is no restriction on the number of objects and how they can be relate to each other. it can clearly be seen with the positions and there relation to each other (a 10x10 map means 100 locations and 4 directions and their relative position defined by the predicate (relative-dir ?location ?loaction ?direction) can be grounded $(100 ^ 2 = 10000)*4=40000$ different ways).
	
	one way to further reduce the grounding problem is to assume that only predicates present (with that particular grounding) and the negated equivalent in the problem description are relevant, until proven otherwise by an effect. The idea is to avoid taking various impossible mutations of static objects into consideration. this is especially effective om problems with a underlying fixed structures, such as maps/levels. the negated predicate is need to that .... walk left impossible, then right still possible
	
	this however leaves how to handle "new" predicates not seen before created by effects. when a new predicate is discovered the negated version is add to all candidate preconditions. this however means that a candidate preconditions cant be proven unless all predicates is discovered. 


	its worth remembering that even though a particular grounding influence if a given action is applicable at that moment, the grounding in it self and the type variables isnt whats interesting... its the relation between the predicates ....therefore generalising of the parameters
	graph representation
	
	to easier detect relations between predicates it is not enough just to know if a given predicate is present, but also what its relation to the other predicates is. A graph representation is therefore used, what is represented is how the objects is related to each other through known predicates.


\subsection{Generating the action schema}
One thing is to acquire knowledge from traces the information also needs to be used, this is where the optimistic and pessimistic approach comes into play. They are used to generate the action schema. Due to the lack of a teacher an the optimistic approach is more advantageous, since it self-sufficient and it is possible to reset the environment.


\textbf{Preconditions}
The preconditions is generated from the narrowed down candidate preconditions, even though that it is not known which part of the candidate preconditions that is the precondition, its know that the particular combination fails. this can be used when generating the action schema, by adding all the candidate set as preconditions. when the sets are reduced enough the number of candidates can compensate for the lack of precision. this also ensures that no mistake is repeated


\textbf{Effects}
everything happens unless proven otherwise,(every possible effect are applied unless the effect is disproven) this can be meaningless sometimes since the effects of not x and x would remove each other. To avoid this problem, each time both effects are applicable one is chosen by 50/50 chance.
\subsection{how does it work?}
Applying the learning approach using optimistic works quiet badly due to how fast downward, handles the large number of unrestricted parameters, in the earlier stages of the learning. It is in particular the preprocessing which takes to long\ref{opt-fd}. This is not a problem solely for Fast downward, the YAHSP\footnote{\url{http://v.vidal.free.fr/onera/\#yahsp}} planner have the same problem except it dies on 4 parameters instead of 5\ref{opt-ya}.
\begin{figure}
	\label{opt-fd}
	\centering
	2 param, no prem, 1 effect, ?? sek\\
	3 param, no prem, 1 effect, ?? sek\\
	4 param, no prem, 1 effect, 48 sek\\
	5 param, no prem, 1 effect, xx sek\\

	all params unristriced\\
	one effect goal state\\
\end{figure}
\begin{figure}
	YAHSP\\
	\label{opt-ya}
	\centering
	2 param, no prem, 1 effect, 0.01 sek 90 actions\\
	3 param, no prem, 1 effect, 0.03 sek 8000 actions\\
	4 param, no prem, 1 effect, 1.49 sek 729000 actions\\
	5 param, no prem, 1 effect, xx sek\\
	
	all params unrestricted\\
	one effect goal state\\
\end{figure}


This means that if learning is going to be applied using fast downward it needs to be a more pessimistic approach, that restricts the parameters and thereby simplifies the preprocessing. Using a completely pessimistic approach leaves one problem, that there is in fact no teacher to ask for help. Some traces can be obtained from the tutorials, but they are not complete enough to solely rely on. if they are processed in order they occurred (falling is missing in the first tutorial).  

how are pessimistic schema generated
	all possible preconditions
	all proven effects

so to solve the problem the golden road is somewhere in between. the idea is to relax the pessimistic approach little by little when it cant find a solution, and thereby ensuring that some restrictions still exists. This is done by adding effects and removing preconditions. The effects add are selected from what effects could have been masked and each precondition is removed with a likely hood of 1/n (proven preconditions cant be removed)
the main disadvantages of this approach is that some of the pessimistic approach still remains, which means that its possible that it turns up no plan and no new information can be learned from that try. to minimize the chance of this happening often the chance of changes increase with each no plan. 
 

\subsection{Extending the domain}
\subsubsection{Conditionals}	
%	simpliefie by only allowing destintingt effects?

	to further extend the support for the domain and allow a wider array of actions such as (sliding?).... Conditionals and Disjunction needs to be add... a working domain without these features can be created, but doing so would defeat the purpose of the learning
	quickly become unreadable to a human 
	move away from directly representing the controls/commands available in the domain used for controlling the avatar

	assumption any given effect can at most appear once as an effect in each action.

	When looking at conditionals and disjunction they both have properties that they can be compiled away by splitting the action into multiple actions. %This can be done due each action having a finite set of outcomes
	
	This can done by moving the condition from effects to precondition and treating it as a non conditional action. at the same time adding conditions so only one of the new actions is applicable at any time.
	
	if a conditional have multiple effects they can also be considered separately...
	%example (better example)
	$p \land q \implies x \land z$
	is equivalent to 
	$(p \land q \implies x)
	\land
	(p \land q \implies z)$
	
	%this far down is how ever to be avoid since it means learning the exact same condition twice
	
	this means that each effect of an action can be considered as a separate sub-action with a single effect. so as soon as a given effect is present in a trace it can be used for deducing the precondition for that effect. The trick is then to merge all the separate sub-actions when generating the action schema, for the pessimistic approach this is done by using the intersection of all candidate preconditions as preconditions and the difference as conditionals.
	
	the speed of learning an action with or without conditionals isnt effect (neither in time or plan attempts). this is due to the extra work add are dependent on the number of effects which in praxis must be considered negligible. furthermore the learning builds on the same principals ...  worst case would be that that the action soley consist of conditionals.... only learning for a single effect each time.... sub actions equivalent to action.... therefore no different than learning separate actions.. 
	

\subsubsection{Disjunction}
	The base (non disjunction part) of an action can be deduced as earlier described (section \ref{basic}), the disjunction part cant be found this way, since the different parts of the disjunction would eliminate each other. Using this approach, it is however possible to detect if a disjunctions appears in the action, this can be seen when contradictions happens (an action failing when fulfilling the predicted precondition). When such a contradiction to the base happens it is guaranteed that at least one predicate from the precondition is not presents. When only looking at the simplest version of disjunctions of the form $ x \lor y \lor z...$ with a maximum of a single disjunction per. action. This simple disjunction can be deduced by using that all present predicates from a contradiction traces cant be part of the disjunction, since it would otherwise have succeed. Furthermore since it possible to treat each effect as an separate sub-action it is also possible using the same approach to deduce a single disjunction for each sub-action, which means a maximum of one disjunction precondition and one disjunction for each conditional. 
	
	This approach however falls apart as soon as more or more complex (eg nested conjunction) disjunction are allowed. The main problem here is that the used approach relies on that no predicates from the disjunction being present on failure where the base is fulfilled, and this can no longer be guaranteed eg. $w \lor x V (y \land z )$ a failure with y present would remove y. 
	
	To avoid this problem it is necessary to identify which part of the disjunctions are related to the failure or success. this is however difficult to determine due to uncertainty add by the disjunctions. The uncertainty is rooted in that multiple disjunctions can hide each others "effects". when looking at a failure its therefore not possible to eliminate any predicates, since potentially any combination of the non present predicates could be part of a disjunction and therefore the cause of the failure. The only thing that can be conclude is that particular complete set of preconditions isnt allowed. On a success the same problem occurs. so without a way to connect course and effect its basically down to something is missing from some disjunction. In the same note adding more complexed disjunctions to the domain also makes it nearly impossible to determine when an action is completely learned. this is due to that every combination of preconditions needs to be tried, unlike earlier where preconditions could be removed if not present a single time. This means that a different approach is need for action schemas using more complex disjunctions, however the simple approach should be sufficient to interact with the Astro Kid domain.
	 
	more advanced disjunction leads to a violation of the assumption...
	

%	
%	avoid storing every single episode since complete knowledge would be need
	


	the disjunction introduces a new aspect, that effects the learning process.  which is that it is no longer possible to discard already learnt episodes,  without the risk of loosing knowledge. The problem lies in that knowledge are obtained from contractions to the base, and some of these cant be discovered until more knowledge is obtained. 
	
	this knowledge is discard since.... optimistic approach.... negligible knowledge loss.
	
	
\section{Generating the action schema}	
	
	The basic structure for generating the actions remains the same, the differences lies in that multiple sub-actions needs to be merged into a single one. This can be done by taking the intersection of the sub-actions preconditions as the preconditions, and having the difference as the conditionals(figure \ref{merge}). Whit the additions of disjunctions the approach moves slightly more away from the completely pessimistic approach. this is due to the disjunction part being deduced using an optimistic approach. this is resulting in the generated disjunction being a disjunction of all possible disjunction predicates.
\begin{figure}
	\label{merge}
		\begin{lstlisting}
subaction 1
pre: (and (at ?p1 ?p2) (relativ_dir ?p2 ?p3 right) 
	(relativ_dir ?p3 ?p4 down))
effect: (and (at ?p1 ?p3))
		
subaction 2
pre: (and (at ?p1 ?p2) (relativ_dir ?p2 ?p3 right) 
	(relativ_dir ?p3 ?p4 down) (clear ?p4))
effect: (and (falling ?p1))
		
merged action
pre: (and (at ?p1 ?p2) (relativ_dir ?p2 ?p3 right) 
	(relativ_dir ?p3 ?p4 down))
effect: (and 
	(at ?p1 ?p3)
	(when (clear ?p4) 
		(falling ?p1)
	)
)
\end{lstlisting}
\end{figure}	

	
	
% quantifiers....
	as with the more complex disjunctions the main problem is to get a handle on it.... a place to start...	
	
%	exstential quantifiers...

the simple use of quantifiers in the domain can easily be deuced.... if an single predicate exists....

%	univarsal quantifiers...
	detect effects not associated with parameters.... special cases 
	
\subsection{space complexity....}

	
\subsection{runnig time /performance /space...}
	
\subsection{how does it work?}	
	
	the consequences of learning conditionals have no effect on how many steps it takes to learn a action
	
	extra time used for deducing since its dependent on the number of sub actions